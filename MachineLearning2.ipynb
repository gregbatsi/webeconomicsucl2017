{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/sklearn/lda.py:4: DeprecationWarning: lda.LDA has been moved to discriminant_analysis.LinearDiscriminantAnalysis in 0.17 and will be removed in 0.19\n",
      "  \"in 0.17 and will be removed in 0.19\", DeprecationWarning)\n",
      "/opt/conda/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from random import randint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.nonparametric.kde import KDEUnivariate\n",
    "from statsmodels.nonparametric import smoothers_lowess\n",
    "from pandas import Series, DataFrame\n",
    "from patsy import dmatrices\n",
    "from sklearn import datasets, svm\n",
    "\n",
    "#import pylab as plt\n",
    "\n",
    "# Set the global default size of matplotlib figures\n",
    "plt.rc('figure', figsize=(10, 5))\n",
    "\n",
    "# Size of matplotlib figures that contain subplots\n",
    "fizsize_with_subplots = (10, 10)\n",
    "\n",
    "# Size of matplotlib histogram bins\n",
    "bin_size = 10\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model, tree, lda, naive_bayes\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from patsy import dmatrices\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "%matplotlib inline\n",
    "import csv\n",
    "import random\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import math\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from random import randint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.nonparametric.kde import KDEUnivariate\n",
    "from statsmodels.nonparametric import smoothers_lowess\n",
    "from pandas import Series, DataFrame\n",
    "from patsy import dmatrices\n",
    "from sklearn import datasets, svm\n",
    "\n",
    "#import pylab as plt\n",
    "\n",
    "# Set the global default size of matplotlib figures\n",
    "plt.rc('figure', figsize=(10, 5))\n",
    "\n",
    "# Size of matplotlib figures that contain subplots\n",
    "fizsize_with_subplots = (10, 10)\n",
    "\n",
    "# Size of matplotlib histogram bins\n",
    "bin_size = 10\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_file4 = \"dataset/train_hot.csv\"\n",
    "train_hot = pd.read_csv(input_file4, dtype=\"unicode\",delimiter='\\t',nrows=20000)\n",
    "#train_hot_20=train_hot.sample(n=1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fit_model(train_hot, model='logistic_regression', max_depth=5):\n",
    "    train_hot.fillna(value=0,inplace=True)\n",
    "    y = train_hot['click']\n",
    "    x = train_hot.drop(['click', 'creative','domain'], axis=1)\n",
    "        # fit the model\n",
    "    if model == 'logistic_regression':\n",
    "        m = linear_model.LogisticRegression()\n",
    "        m = m.fit(x, y)\n",
    "        a=m.score(x,y)\n",
    "    elif model == 'tree':\n",
    "        m = tree.DecisionTreeClassifier(max_depth=max_depth)\n",
    "        m.fit(x, y)\n",
    "        a=m.score(x,y)\n",
    "    elif model == 'lda':\n",
    "        m = lda.LDA()\n",
    "        m.fit(x, y)\n",
    "        a=m.score(x,y)\n",
    "    elif model == 'naivebayes':\n",
    "        m = naive_bayes.GaussianNB()\n",
    "        m.fit(x, y)\n",
    "        a=m.score(x,y)\n",
    "    print(a)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_model(train_hot,'logistic_regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7713\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_model(train_hot,'naivebayes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99935\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_model(train_hot,'tree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/sklearn/discriminant_analysis.py:389: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99105\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "              solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_model(train_hot,'lda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_file5 = \"dataset/val_hot.csv\"\n",
    "val_hot = pd.read_csv(input_file4, dtype=\"unicode\",delimiter='\\t',nrows=20000)\n",
    "y = train_hot['click']\n",
    "x = train_hot.drop(['click', 'creative','domain'], axis=1)\n",
    "y_test = val_hot['click']\n",
    "x_test = val_hot.drop(['click', 'creative','domain'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model by splitting into train and test sets\n",
    "model2 = LogisticRegression()\n",
    "model2.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '0' '0' ..., '0' '0' '0']\n"
     ]
    }
   ],
   "source": [
    "predicted = model2.predict(x_test)\n",
    "print (predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9.99015879e-01   9.84120676e-04]\n",
      " [  9.99843146e-01   1.56853887e-04]\n",
      " [  9.98929082e-01   1.07091820e-03]\n",
      " ..., \n",
      " [  9.95675167e-01   4.32483333e-03]\n",
      " [  9.99853456e-01   1.46544168e-04]\n",
      " [  9.99773508e-01   2.26491683e-04]]\n"
     ]
    }
   ],
   "source": [
    "probs = model2.predict_proba(x_test)\n",
    "print (probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99915\n"
     ]
    }
   ],
   "source": [
    "# generate evaluation metrics\n",
    "print (metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19983     0]\n",
      " [   17     0]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     19983\n",
      "          1       0.00      0.00      0.00        17\n",
      "\n",
      "avg / total       1.00      1.00      1.00     20000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print (metrics.confusion_matrix(y_test, predicted))\n",
    "print (metrics.classification_report(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.9990005   0.9990005   0.9990005   0.999       0.999       0.999       0.999\n",
      "  0.99949975  0.99949975  0.99949975]\n",
      "0.999150074888\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model using 10-fold cross-validation\n",
    "scores = cross_val_score(LogisticRegression(), x, y, scoring='accuracy', cv=10)\n",
    "print (scores)\n",
    "print (scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
